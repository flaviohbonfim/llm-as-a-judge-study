# üöÄ Quick Start: LLMs as a Judge

Guia r√°pido para come√ßar a usar LLM Judge em 5 minutos.

## Pr√©-requisitos

```bash
# Instalar depend√™ncias
poetry add langfuse

# Ou com pip
pip install langfuse
```

## Passo 1: Configura√ß√£o B√°sica

```python
from google.adk import Agent, Runner
from langfuse import Langfuse
from examples.llm_judge_implementation import LangfuseLLMJudge

# 1. Configure o judge agent
judge_agent = Agent(
    name="evaluation_judge",
    description="Especialista em avaliar agentes de IA",
    instruction="""
    Voc√™ √© um juiz especializado em avaliar agentes de IA.
    Sempre forne√ßa avalia√ß√µes em JSON estruturado com scores, justificativas e recomenda√ß√µes.
    """,
    model="gemini-2.0-flash"  # Recomendado: melhor custo-benef√≠cio
)

# 2. Configure Langfuse (opcional, mas recomendado)
langfuse = Langfuse(
    secret_key="sk-lf-...",  # Sua chave secreta
    public_key="pk-lf-...",  # Sua chave p√∫blica
    host="https://cloud.langfuse.com"
)

# 3. Crie o judge
runner = Runner()
judge = LangfuseLLMJudge(
    judge_agent=judge_agent,
    runner=runner,
    langfuse_client=langfuse
)
```

## Passo 2: Avaliar uma Resposta

```python
import asyncio

async def avaliar_resposta():
    evaluation = await judge.evaluate_response(
        user_query="O que √© Python?",
        agent_response="Python √© uma linguagem de programa√ß√£o de alto n√≠vel.",
        expected_response="Python √© uma linguagem de programa√ß√£o interpretada..."
    )
    
    print(f"‚úÖ Score: {evaluation['score']:.2f}")
    print(f"üìù Justificativa: {evaluation['justification']}")
    print(f"‚ú® Pontos fortes: {evaluation.get('strengths', [])}")
    print(f"‚ö†Ô∏è  Pontos fracos: {evaluation.get('weaknesses', [])}")

# Executar
asyncio.run(avaliar_resposta())
```

## Passo 3: Avaliar Trajet√≥ria

```python
async def avaliar_trajetoria():
    trajectory_eval = await judge.evaluate_trajectory(
        expected_trajectory=["search", "retrieve", "generate"],
        actual_trajectory=["search", "retrieve", "generate", "validate"]
    )
    
    print(f"‚úÖ Score da Trajet√≥ria: {trajectory_eval['score']:.2f}")
    print(f"üìä Completude: {trajectory_eval.get('completeness', 0):.2f}")
    print(f"‚ö° Efici√™ncia: {trajectory_eval.get('efficiency', 0):.2f}")

asyncio.run(avaliar_trajetoria())
```

## Passo 4: Comparar M√∫ltiplas Respostas

```python
async def comparar_respostas():
    comparison = await judge.compare_responses(
        user_query="Explique machine learning",
        responses=[
            {"label": "GPT-4", "response": "Machine learning √©..."},
            {"label": "Gemini", "response": "Machine learning √©..."},
            {"label": "Claude", "response": "Machine learning √©..."}
        ]
    )
    
    print(f"üèÜ Melhor resposta: {comparison['winner']}")
    print(f"üìä Scores: {comparison['scores']}")
    print(f"üí≠ Racioc√≠nio: {comparison['reasoning']}")

asyncio.run(comparar_respostas())
```

## Passo 5: Integrar com Seu Agente

```python
from google.adk import Agent, Runner, Session

# Seu agente
meu_agente = Agent(
    name="meu_agente",
    description="...",
    instruction="...",
    model="gemini-2.0-flash"
)

async def avaliar_meu_agente():
    runner = Runner()
    session = Session()
    
    # Executa seu agente
    user_query = "Qual √© a capital do Brasil?"
    response = await runner.run(
        agent=meu_agente,
        session=session,
        user_content=user_query
    )
    
    # Avalia com judge
    evaluation = await judge.evaluate_response(
        user_query=user_query,
        agent_response=response.content
    )
    
    print(f"Resposta do agente: {response.content}")
    print(f"Score: {evaluation['score']:.2f}")
    
    return evaluation

asyncio.run(avaliar_meu_agente())
```

## Exemplo Completo: Pipeline de Avalia√ß√£o

```python
async def pipeline_completo():
    """Pipeline completo de avalia√ß√£o"""
    
    # Casos de teste
    test_cases = [
        {
            "user_query": "O que √© Python?",
            "expected_response": "Python √© uma linguagem de programa√ß√£o..."
        },
        {
            "user_query": "Explique machine learning",
            "expected_response": "Machine learning √© um subcampo da IA..."
        }
    ]
    
    results = []
    
    for test_case in test_cases:
        # Executa agente (substitua pelo seu agente real)
        agent_response = "..."  # Resposta do seu agente
        
        # Avalia
        evaluation = await judge.evaluate_response(
            user_query=test_case["user_query"],
            agent_response=agent_response,
            expected_response=test_case.get("expected_response")
        )
        
        results.append({
            "test_case": test_case["user_query"],
            "score": evaluation["score"],
            "evaluation": evaluation
        })
    
    # Resumo
    avg_score = sum(r["score"] for r in results) / len(results)
    print(f"üìä Score M√©dio: {avg_score:.2f}")
    print(f"‚úÖ Testes Passados: {sum(1 for r in results if r['score'] >= 0.7)}/{len(results)}")
    
    return results

asyncio.run(pipeline_completo())
```

## Configura√ß√£o Avan√ßada

### Usar Crit√©rios Customizados

```python
custom_criteria = {
    "correctness": "A resposta est√° correta?",
    "relevance": "A resposta √© relevante?",
    "completeness": "A resposta est√° completa?",
    "clarity": "A resposta √© clara?",
    "safety": "A resposta √© segura?"
}

judge = LangfuseLLMJudge(
    judge_agent=judge_agent,
    runner=runner,
    langfuse_client=langfuse,
    evaluation_criteria=custom_criteria
)
```

### Usar Modelo Diferente

```python
# Para casos cr√≠ticos, use modelo mais poderoso
judge_agent_critical = Agent(
    name="evaluation_judge_critical",
    description="...",
    instruction="...",
    model="gemini-2.0-pro"  # ou "gpt-4o"
)
```

### Sem Langfuse (Modo Simples)

```python
from examples.llm_judge_implementation import LLMJudge

# Judge sem Langfuse
judge_simple = LLMJudge(
    judge_agent=judge_agent,
    runner=runner
)
```

## Troubleshooting

### Erro: "N√£o foi poss√≠vel extrair JSON"
- O judge pode n√£o estar retornando JSON v√°lido
- Solu√ß√£o: Verifique o prompt do judge agent

### Erro: "Langfuse connection failed"
- Langfuse n√£o est√° configurado corretamente
- Solu√ß√£o: Use `LLMJudge` sem Langfuse ou verifique credenciais

### Avalia√ß√µes Inconsistentes
- Modelos podem variar entre execu√ß√µes
- Solu√ß√£o: Use `temperature=0.0` no judge agent

## Pr√≥ximos Passos

1. ‚úÖ Leia o estudo completo: `docs/LLMs_as_Judge_Study.md`
2. ‚úÖ Explore exemplos: `examples/llm_judge_implementation.py`
3. ‚úÖ Configure Langfuse para observabilidade
4. ‚úÖ Adapte para seus casos de uso espec√≠ficos

## Recursos

- üìö **Estudo Completo**: `docs/LLMs_as_Judge_Study.md`
- üìù **Resumo Executivo**: `docs/RESUMO_EXECUTIVO_LLMs_Judge.md`
- üíª **Exemplos**: `examples/`
- ‚öôÔ∏è **Configura√ß√µes**: `examples/judge_configs.yaml`

---

**Pronto para come√ßar!** üöÄ

