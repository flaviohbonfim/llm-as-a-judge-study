# üéØ Estudo Profundo: LLMs as a Judge
## Identificando os Melhores Tipos de LLMs para Avalia√ß√£o de Agentes

**Data:** 2024  
**Contexto:** Projetos Python com Google ADK e Langfuse  
**Objetivo:** Identificar os melhores tipos de LLMs para atuar como ju√≠zes na avalia√ß√£o de agentes de IA

---

## üìã Sum√°rio Executivo

Este documento apresenta um estudo abrangente sobre o uso de Large Language Models (LLMs) como ju√≠zes (judges) na avalia√ß√£o de agentes de IA. O estudo foca em:

- **Conceito de LLMs as a Judge**: Modelos que avaliam a qualidade de respostas, trajet√≥rias e comportamento de agentes
- **Tipos de Avalia√ß√£o**: Trajet√≥ria de ferramentas, qualidade de resposta, comportamento do agente
- **Modelos Recomendados**: An√°lise comparativa de diferentes LLMs para fun√ß√£o de judge
- **Integra√ß√£o Pr√°tica**: Como integrar judges com Google ADK e Langfuse
- **M√©tricas e Benchmarks**: Como medir a efic√°cia dos judges
- **Melhores Pr√°ticas**: Padr√µes e recomenda√ß√µes para implementa√ß√£o

---

## 1. Introdu√ß√£o ao Conceito: LLMs as a Judge

### 1.1 O que s√£o LLMs as a Judge?

**LLMs as a Judge** √© uma t√©cnica onde um modelo de linguagem grande √© usado para avaliar a qualidade, corre√ß√£o e adequa√ß√£o de respostas geradas por outros modelos ou agentes. Em vez de usar m√©tricas tradicionais baseadas em regras ou ground truth humano, o judge LLM atua como um avaliador inteligente que pode:

- Avaliar a qualidade sem√¢ntica de respostas
- Comparar m√∫ltiplas respostas e ranquear
- Avaliar trajet√≥rias de agentes (sequ√™ncia de a√ß√µes)
- Detectar problemas de seguran√ßa, alinhamento ou qualidade
- Fornecer feedback estruturado e explic√°vel

### 1.2 Por que usar LLMs como Judge?

#### Vantagens:

1. **Escalabilidade**: Avalia milhares de respostas automaticamente
2. **Consist√™ncia**: Aplica os mesmos crit√©rios de avalia√ß√£o
3. **Custo**: Mais barato que avalia√ß√£o humana em larga escala
4. **Velocidade**: Avalia√ß√µes em tempo real ou quase real
5. **Flexibilidade**: Pode avaliar m√∫ltiplos aspectos simultaneamente
6. **Explicabilidade**: Pode fornecer justificativas para suas avalia√ß√µes

#### Desafios:

1. **Vi√©s do Modelo**: O judge pode ter seus pr√≥prios vieses
2. **Alinhamento**: Necessidade de garantir que o judge avalia o que realmente importa
3. **Consist√™ncia**: Pode haver varia√ß√£o entre avalia√ß√µes
4. **Custo Computacional**: Requer chamadas adicionais a LLMs
5. **Ground Truth**: Ainda pode ser necess√°rio valida√ß√£o humana

### 1.3 Tipos de Avalia√ß√£o com LLMs as Judge

#### 1.3.1 Avalia√ß√£o de Trajet√≥ria (Trajectory Evaluation)

Avalia a sequ√™ncia de a√ß√µes que o agente tomou:

- **Ordem das a√ß√µes**: As a√ß√µes foram executadas na ordem correta?
- **Ferramentas utilizadas**: O agente usou as ferramentas apropriadas?
- **Efici√™ncia**: O agente tomou o caminho mais eficiente?
- **Completude**: Todas as a√ß√µes necess√°rias foram executadas?

**Exemplo de Prompt para Judge:**

```python
TRAJECTORY_EVALUATION_PROMPT = """
Voc√™ √© um juiz especializado em avaliar trajet√≥rias de agentes de IA.

Trajet√≥ria Esperada: {expected_trajectory}
Trajet√≥ria Real: {actual_trajectory}

Avalie:
1. As a√ß√µes foram executadas na ordem correta? (0-1)
2. Todas as a√ß√µes necess√°rias foram executadas? (0-1)
3. Alguma a√ß√£o desnecess√°ria foi executada? (0-1)
4. A trajet√≥ria foi eficiente? (0-1)

Forne√ßa uma pontua√ß√£o geral de 0-1 e uma justificativa detalhada.
"""
```

#### 1.3.2 Avalia√ß√£o de Qualidade de Resposta (Response Quality Evaluation)

Avalia a qualidade da resposta final do agente:

- **Corre√ß√£o**: A resposta est√° factualmente correta?
- **Relev√¢ncia**: A resposta responde √† pergunta do usu√°rio?
- **Completude**: A resposta est√° completa?
- **Clareza**: A resposta √© clara e bem estruturada?
- **Seguran√ßa**: A resposta √© segura e apropriada?

**Exemplo de Prompt para Judge:**

```python
RESPONSE_QUALITY_PROMPT = """
Voc√™ √© um juiz especializado em avaliar respostas de agentes de IA.

Pergunta do Usu√°rio: {user_query}
Resposta do Agente: {agent_response}
Contexto: {context}

Avalie a resposta em:
1. Corre√ß√£o factual (0-1)
2. Relev√¢ncia √† pergunta (0-1)
3. Completude (0-1)
4. Clareza e estrutura (0-1)
5. Seguran√ßa e apropria√ß√£o (0-1)

Forne√ßa uma pontua√ß√£o geral de 0-1 e feedback detalhado.
"""
```

#### 1.3.3 Avalia√ß√£o Comparativa (Comparative Evaluation)

Compara m√∫ltiplas respostas e ranqueia:

- **Ranking**: Qual resposta √© melhor?
- **Diferen√ßas**: Quais s√£o as diferen√ßas principais?
- **Trade-offs**: Quais s√£o os pr√≥s e contras de cada resposta?

**Exemplo de Prompt para Judge:**

```python
COMPARATIVE_EVALUATION_PROMPT = """
Voc√™ √© um juiz especializado em comparar respostas de agentes.

Pergunta: {user_query}
Resposta A: {response_a}
Resposta B: {response_b}

Compare as respostas e:
1. Identifique qual √© melhor (A ou B)
2. Forne√ßa uma pontua√ß√£o relativa (0-1 para cada)
3. Liste os pontos fortes e fracos de cada resposta
4. Explique sua decis√£o
"""
```

#### 1.3.4 Avalia√ß√£o de Comportamento (Behavioral Evaluation)

Avalia o comportamento geral do agente:

- **Alinhamento**: O agente seguiu as instru√ß√µes?
- **√âtica**: O comportamento foi √©tico?
- **Robustez**: O agente lidou bem com edge cases?
- **Consist√™ncia**: O comportamento foi consistente?

---

## 2. Modelos LLM Adequados para Fun√ß√£o de Judge

### 2.1 Crit√©rios para Sele√ß√£o de Judge LLM

Ao escolher um LLM para fun√ß√£o de judge, considere:

1. **Capacidade de Racioc√≠nio**: Precisa entender nuances e contexto
2. **Consist√™ncia**: Deve fornecer avalia√ß√µes consistentes
3. **Capacidade de Seguir Instru√ß√µes**: Deve seguir prompts de avalia√ß√£o precisamente
4. **Custo**: Deve ser economicamente vi√°vel para uso em escala
5. **Lat√™ncia**: Deve ser r√°pido o suficiente para uso em produ√ß√£o
6. **Disponibilidade**: Deve estar dispon√≠vel atrav√©s de APIs confi√°veis
7. **Capacidade de Output Estruturado**: Deve poder fornecer avalia√ß√µes estruturadas

### 2.2 An√°lise Comparativa de Modelos

#### 2.2.1 Modelos GPT (OpenAI)

**GPT-4 Turbo / GPT-4o**
- ‚úÖ Excelente capacidade de racioc√≠nio
- ‚úÖ Alta consist√™ncia
- ‚úÖ Suporte a JSON mode para outputs estruturados
- ‚úÖ Boa capacidade de seguir instru√ß√µes complexas
- ‚ö†Ô∏è Custo mais alto
- ‚ö†Ô∏è Lat√™ncia moderada

**GPT-3.5 Turbo**
- ‚úÖ Custo mais baixo
- ‚úÖ Lat√™ncia baixa
- ‚úÖ Boa capacidade de racioc√≠nio
- ‚ö†Ô∏è Menos consistente que GPT-4
- ‚ö†Ô∏è Pode ter dificuldade com avalia√ß√µes muito complexas

**Recomenda√ß√£o**: GPT-4o para avalia√ß√µes cr√≠ticas, GPT-3.5 Turbo para avalia√ß√µes em larga escala.

#### 2.2.2 Modelos Gemini (Google)

**Gemini 2.0 Flash**
- ‚úÖ Excelente custo-benef√≠cio
- ‚úÖ Lat√™ncia muito baixa
- ‚úÖ Boa capacidade de racioc√≠nio
- ‚úÖ Integra√ß√£o nativa com Google ADK
- ‚ö†Ô∏è Pode ser menos consistente que GPT-4 em casos complexos

**Gemini 2.0 Pro**
- ‚úÖ Excelente capacidade de racioc√≠nio
- ‚úÖ Alta consist√™ncia
- ‚úÖ Suporte a contexto muito longo
- ‚ö†Ô∏è Custo mais alto que Flash
- ‚ö†Ô∏è Lat√™ncia maior

**Recomenda√ß√£o**: Gemini 2.0 Flash para maioria dos casos, Gemini 2.0 Pro para avalia√ß√µes muito complexas.

#### 2.2.3 Modelos Claude (Anthropic)

**Claude 3.5 Sonnet**
- ‚úÖ Excelente capacidade de racioc√≠nio
- ‚úÖ Muito consistente
- ‚úÖ Excelente em an√°lise detalhada
- ‚úÖ Suporte a contexto muito longo
- ‚ö†Ô∏è Custo moderado-alto
- ‚ö†Ô∏è Lat√™ncia moderada

**Claude 3 Haiku**
- ‚úÖ Custo muito baixo
- ‚úÖ Lat√™ncia muito baixa
- ‚úÖ Boa capacidade de racioc√≠nio
- ‚ö†Ô∏è Menos consistente que Sonnet

**Recomenda√ß√£o**: Claude 3.5 Sonnet para avalia√ß√µes cr√≠ticas, Claude 3 Haiku para avalia√ß√µes em larga escala.

#### 2.2.4 Modelos Open Source

**Llama 3.1 70B / 405B**
- ‚úÖ Custo muito baixo (self-hosted)
- ‚úÖ Controle total sobre o modelo
- ‚úÖ Sem limites de rate
- ‚ö†Ô∏è Requer infraestrutura pr√≥pria
- ‚ö†Ô∏è Pode ser menos consistente que modelos comerciais
- ‚ö†Ô∏è Requer fine-tuning para melhor performance

**Mixtral 8x7B / 8x22B**
- ‚úÖ Custo muito baixo (self-hosted)
- ‚úÖ Boa capacidade de racioc√≠nio
- ‚ö†Ô∏è Requer infraestrutura pr√≥pria
- ‚ö†Ô∏è Pode precisar de fine-tuning

**Recomenda√ß√£o**: Para organiza√ß√µes com infraestrutura adequada e necessidade de controle total.

### 2.3 Tabela Comparativa Resumida

| Modelo | Custo | Lat√™ncia | Racioc√≠nio | Consist√™ncia | Recomenda√ß√£o |
|--------|-------|----------|------------|--------------|--------------|
| GPT-4o | Alto | M√©dia | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Cr√≠tico |
| GPT-3.5 Turbo | Baixo | Baixa | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Larga escala |
| Gemini 2.0 Flash | Muito Baixo | Muito Baixa | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | **Recomendado** |
| Gemini 2.0 Pro | M√©dio | M√©dia | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Complexo |
| Claude 3.5 Sonnet | M√©dio-Alto | M√©dia | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Cr√≠tico |
| Claude 3 Haiku | Muito Baixo | Muito Baixa | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Larga escala |
| Llama 3.1 70B | Muito Baixo* | M√©dia* | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Self-hosted |

*Custo e lat√™ncia dependem da infraestrutura pr√≥pria

### 2.4 Recomenda√ß√µes por Caso de Uso

#### Para Avalia√ß√µes em Tempo Real (Produ√ß√£o)
- **Prim√°rio**: Gemini 2.0 Flash ou Claude 3 Haiku
- **Alternativa**: GPT-3.5 Turbo

#### Para Avalia√ß√µes Cr√≠ticas (Qualidade M√°xima)
- **Prim√°rio**: GPT-4o ou Claude 3.5 Sonnet
- **Alternativa**: Gemini 2.0 Pro

#### Para Avalia√ß√µes em Larga Escala (Batch)
- **Prim√°rio**: Gemini 2.0 Flash ou Claude 3 Haiku
- **Alternativa**: GPT-3.5 Turbo

#### Para Avalia√ß√µes com Alto Volume e Baixo Custo
- **Prim√°rio**: Modelos open source self-hosted (Llama 3.1)
- **Alternativa**: Gemini 2.0 Flash

---

## 3. Integra√ß√£o com Google ADK

### 3.1 Arquitetura de Integra√ß√£o

O Google ADK j√° possui suporte nativo para avalia√ß√£o atrav√©s do `AgentEvaluator`. Podemos estender isso para usar LLMs as Judge:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Agent ADK     ‚îÇ
‚îÇ   (Sob Teste)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AgentEvaluator ‚îÇ
‚îÇ   (ADK Native)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Judge      ‚îÇ
‚îÇ  (Customizado)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Langfuse      ‚îÇ
‚îÇ  (Tracing/Log)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Implementa√ß√£o de Judge com ADK

#### 3.2.1 Criando um Judge Agent

```python
from google.adk import Agent
from typing import Dict, Any, List
import json

# Judge Agent especializado em avalia√ß√£o
judge_agent = Agent(
    name="evaluation_judge",
    description="Especialista em avaliar qualidade de respostas e trajet√≥rias de agentes",
    instruction="""
    Voc√™ √© um juiz especializado em avaliar agentes de IA.
    
    Sua tarefa √© avaliar:
    1. A qualidade da resposta do agente
    2. A trajet√≥ria de a√ß√µes tomadas
    3. A adequa√ß√£o ao contexto e requisitos
    
    Sempre forne√ßa:
    - Uma pontua√ß√£o num√©rica de 0-1
    - Uma justificativa detalhada
    - Pontos fortes e fracos identificados
    - Recomenda√ß√µes de melhoria
    
    Formate sua resposta como JSON estruturado.
    """,
    model="gemini-2.0-flash",  # Ou outro modelo adequado
)
```

#### 3.2.2 Classe de Judge Customizada

```python
from google.adk import Runner, Session
from typing import Optional, Dict, Any
import json

class LLMJudge:
    """Judge usando LLM para avaliar agentes ADK"""
    
    def __init__(
        self,
        judge_agent: Agent,
        runner: Runner,
        evaluation_criteria: Dict[str, str]
    ):
        self.judge_agent = judge_agent
        self.runner = runner
        self.criteria = evaluation_criteria
    
    async def evaluate_trajectory(
        self,
        expected_trajectory: List[str],
        actual_trajectory: List[str],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Avalia a trajet√≥ria de a√ß√µes do agente"""
        
        prompt = f"""
        Avalie a trajet√≥ria do agente:
        
        Trajet√≥ria Esperada: {json.dumps(expected_trajectory, ensure_ascii=False)}
        Trajet√≥ria Real: {json.dumps(actual_trajectory, ensure_ascii=False)}
        Contexto: {json.dumps(context or {}, ensure_ascii=False)}
        
        Crit√©rios de Avalia√ß√£o:
        {json.dumps(self.criteria, ensure_ascii=False, indent=2)}
        
        Forne√ßa uma avalia√ß√£o em JSON com:
        - score: pontua√ß√£o de 0-1
        - order_match: as a√ß√µes est√£o na ordem correta? (0-1)
        - completeness: todas as a√ß√µes necess√°rias foram executadas? (0-1)
        - efficiency: a trajet√≥ria foi eficiente? (0-1)
        - justification: justificativa detalhada
        - strengths: pontos fortes
        - weaknesses: pontos fracos
        """
        
        session = Session()
        response = await self.runner.run(
            agent=self.judge_agent,
            session=session,
            user_content=prompt
        )
        
        # Parse da resposta JSON
        try:
            evaluation = json.loads(response.content)
            return evaluation
        except json.JSONDecodeError:
            # Fallback: tentar extrair JSON da resposta
            return self._extract_json_from_response(response.content)
    
    async def evaluate_response(
        self,
        user_query: str,
        agent_response: str,
        expected_response: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Avalia a qualidade da resposta do agente"""
        
        prompt = f"""
        Avalie a resposta do agente:
        
        Pergunta do Usu√°rio: {user_query}
        Resposta do Agente: {agent_response}
        Resposta Esperada (refer√™ncia): {expected_response or "N/A"}
        Contexto: {json.dumps(context or {}, ensure_ascii=False, indent=2)}
        
        Crit√©rios de Avalia√ß√£o:
        {json.dumps(self.criteria, ensure_ascii=False, indent=2)}
        
        Forne√ßa uma avalia√ß√£o em JSON com:
        - score: pontua√ß√£o geral de 0-1
        - correctness: corre√ß√£o factual (0-1)
        - relevance: relev√¢ncia √† pergunta (0-1)
        - completeness: completude (0-1)
        - clarity: clareza e estrutura (0-1)
        - safety: seguran√ßa e apropria√ß√£o (0-1)
        - justification: justificativa detalhada
        - strengths: pontos fortes
        - weaknesses: pontos fracos
        - recommendations: recomenda√ß√µes de melhoria
        """
        
        session = Session()
        response = await self.runner.run(
            agent=self.judge_agent,
            session=session,
            user_content=prompt
        )
        
        try:
            evaluation = json.loads(response.content)
            return evaluation
        except json.JSONDecodeError:
            return self._extract_json_from_response(response.content)
    
    async def compare_responses(
        self,
        user_query: str,
        responses: List[Dict[str, str]],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Compara m√∫ltiplas respostas e ranqueia"""
        
        responses_text = "\n\n".join([
            f"Resposta {i+1}:\n{resp.get('response', '')}"
            for i, resp in enumerate(responses)
        ])
        
        prompt = f"""
        Compare e ranqueie as seguintes respostas:
        
        Pergunta: {user_query}
        
        {responses_text}
        
        Contexto: {json.dumps(context or {}, ensure_ascii=False, indent=2)}
        
        Forne√ßa uma compara√ß√£o em JSON com:
        - rankings: lista ordenada de √≠ndices (melhor primeiro)
        - scores: pontua√ß√µes de 0-1 para cada resposta
        - comparison: compara√ß√£o detalhada entre as respostas
        - winner: √≠ndice da melhor resposta
        - reasoning: racioc√≠nio por tr√°s da decis√£o
        """
        
        session = Session()
        response = await self.runner.run(
            agent=self.judge_agent,
            session=session,
            user_content=prompt
        )
        
        try:
            comparison = json.loads(response.content)
            return comparison
        except json.JSONDecodeError:
            return self._extract_json_from_response(response.content)
    
    def _extract_json_from_response(self, text: str) -> Dict[str, Any]:
        """Extrai JSON de uma resposta que pode conter texto adicional"""
        import re
        json_match = re.search(r'\{.*\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        return {"error": "N√£o foi poss√≠vel extrair JSON da resposta", "raw": text}
```

#### 3.2.3 Integra√ß√£o com AgentEvaluator do ADK

```python
from google.adk.evaluation import AgentEvaluator
from typing import List, Dict, Any

class LLMJudgeEvaluator(AgentEvaluator):
    """Extens√£o do AgentEvaluator do ADK com LLM Judge"""
    
    def __init__(
        self,
        agent_to_evaluate: Agent,
        judge: LLMJudge,
        **kwargs
    ):
        super().__init__(agent_to_evaluate, **kwargs)
        self.judge = judge
    
    async def evaluate_with_judge(
        self,
        test_cases: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Avalia casos de teste usando LLM Judge"""
        
        results = []
        
        for test_case in test_cases:
            # Executa o agente sendo avaliado
            agent_result = await self._run_agent(test_case)
            
            # Avalia com o judge
            trajectory_eval = None
            if test_case.get("expected_trajectory"):
                trajectory_eval = await self.judge.evaluate_trajectory(
                    expected_trajectory=test_case["expected_trajectory"],
                    actual_trajectory=agent_result["trajectory"],
                    context=test_case.get("context")
                )
            
            response_eval = await self.judge.evaluate_response(
                user_query=test_case["user_query"],
                agent_response=agent_result["response"],
                expected_response=test_case.get("expected_response"),
                context=test_case.get("context")
            )
            
            results.append({
                "test_case": test_case,
                "agent_result": agent_result,
                "trajectory_evaluation": trajectory_eval,
                "response_evaluation": response_eval,
                "overall_score": self._calculate_overall_score(
                    trajectory_eval,
                    response_eval
                )
            })
        
        return {
            "results": results,
            "summary": self._generate_summary(results)
        }
    
    def _calculate_overall_score(
        self,
        trajectory_eval: Optional[Dict[str, Any]],
        response_eval: Dict[str, Any]
    ) -> float:
        """Calcula pontua√ß√£o geral combinando trajet√≥ria e resposta"""
        
        scores = []
        
        if trajectory_eval:
            scores.append(trajectory_eval.get("score", 0))
        
        if response_eval:
            scores.append(response_eval.get("score", 0))
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Gera resumo das avalia√ß√µes"""
        
        scores = [r["overall_score"] for r in results]
        
        return {
            "total_tests": len(results),
            "average_score": sum(scores) / len(scores) if scores else 0,
            "min_score": min(scores) if scores else 0,
            "max_score": max(scores) if scores else 0,
            "pass_rate": sum(1 for s in scores if s >= 0.7) / len(scores) if scores else 0
        }
```

---

## 4. Integra√ß√£o com Langfuse

### 4.1 Por que Integrar Langfuse?

Langfuse oferece:

- **Tracing**: Rastreamento completo de execu√ß√µes
- **Feedback**: Coleta de feedback humano e autom√°tico
- **Analytics**: An√°lise de performance e custos
- **Prompt Management**: Versionamento e gerenciamento de prompts
- **Scores**: Armazenamento de pontua√ß√µes de avalia√ß√£o

### 4.2 Implementa√ß√£o de Integra√ß√£o

#### 4.2.1 Judge com Tracing Langfuse

```python
from langfuse import Langfuse
from langfuse.decorators import langfuse_context
from typing import Dict, Any, Optional
import asyncio

class LangfuseLLMJudge(LLMJudge):
    """LLM Judge integrado com Langfuse para tracing e analytics"""
    
    def __init__(
        self,
        judge_agent: Agent,
        runner: Runner,
        evaluation_criteria: Dict[str, str],
        langfuse_client: Langfuse
    ):
        super().__init__(judge_agent, runner, evaluation_criteria)
        self.langfuse = langfuse_client
    
    async def evaluate_trajectory(
        self,
        expected_trajectory: List[str],
        actual_trajectory: List[str],
        context: Optional[Dict[str, Any]] = None,
        trace_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Avalia trajet√≥ria com tracing Langfuse"""
        
        # Cria trace no Langfuse
        trace = self.langfuse.trace(
            name="trajectory_evaluation",
            id=trace_id,
            metadata={
                "expected_trajectory": expected_trajectory,
                "actual_trajectory": actual_trajectory,
                "context": context
            }
        )
        
        try:
            # Executa avalia√ß√£o
            evaluation = await super().evaluate_trajectory(
                expected_trajectory,
                actual_trajectory,
                context
            )
            
            # Registra score no Langfuse
            trace.score(
                name="trajectory_score",
                value=evaluation.get("score", 0),
                comment=evaluation.get("justification", "")
            )
            
            # Registra scores individuais
            for metric, value in evaluation.items():
                if isinstance(value, (int, float)) and 0 <= value <= 1:
                    trace.score(
                        name=f"trajectory_{metric}",
                        value=value
                    )
            
            return evaluation
            
        except Exception as e:
            trace.update(
                level="ERROR",
                status_message=str(e)
            )
            raise
    
    async def evaluate_response(
        self,
        user_query: str,
        agent_response: str,
        expected_response: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        trace_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Avalia resposta com tracing Langfuse"""
        
        trace = self.langfuse.trace(
            name="response_evaluation",
            id=trace_id,
            input={
                "user_query": user_query,
                "agent_response": agent_response,
                "expected_response": expected_response
            },
            metadata=context or {}
        )
        
        try:
            evaluation = await super().evaluate_response(
                user_query,
                agent_response,
                expected_response,
                context
            )
            
            # Registra score geral
            trace.score(
                name="response_score",
                value=evaluation.get("score", 0),
                comment=evaluation.get("justification", "")
            )
            
            # Registra scores individuais
            for metric in ["correctness", "relevance", "completeness", "clarity", "safety"]:
                if metric in evaluation:
                    trace.score(
                        name=f"response_{metric}",
                        value=evaluation[metric]
                    )
            
            # Registra pontos fortes e fracos como observa√ß√µes
            if evaluation.get("strengths"):
                trace.observation(
                    name="strengths",
                    value=evaluation["strengths"]
                )
            
            if evaluation.get("weaknesses"):
                trace.observation(
                    name="weaknesses",
                    value=evaluation["weaknesses"]
                )
            
            return evaluation
            
        except Exception as e:
            trace.update(
                level="ERROR",
                status_message=str(e)
            )
            raise
```

#### 4.2.2 Feedback Loop com Langfuse

```python
class LangfuseFeedbackJudge:
    """Integra feedback humano com avalia√ß√£o autom√°tica do judge"""
    
    def __init__(
        self,
        judge: LangfuseLLMJudge,
        langfuse_client: Langfuse
    ):
        self.judge = judge
        self.langfuse = langfuse_client
    
    async def evaluate_with_feedback(
        self,
        trace_id: str,
        user_query: str,
        agent_response: str,
        human_feedback: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Combina avalia√ß√£o autom√°tica com feedback humano"""
        
        # Avalia√ß√£o autom√°tica
        auto_eval = await self.judge.evaluate_response(
            user_query=user_query,
            agent_response=agent_response,
            trace_id=trace_id
        )
        
        # Se houver feedback humano, combina
        if human_feedback:
            combined_score = self._combine_scores(
                auto_eval.get("score", 0),
                human_feedback.get("score", 0)
            )
            
            # Registra feedback humano no Langfuse
            self.langfuse.score(
                trace_id=trace_id,
                name="human_feedback",
                value=human_feedback.get("score", 0),
                comment=human_feedback.get("comment", "")
            )
            
            # Registra score combinado
            self.langfuse.score(
                trace_id=trace_id,
                name="combined_score",
                value=combined_score,
                comment="Combina√ß√£o de avalia√ß√£o autom√°tica e feedback humano"
            )
            
            auto_eval["human_feedback"] = human_feedback
            auto_eval["combined_score"] = combined_score
        
        return auto_eval
    
    def _combine_scores(
        self,
        auto_score: float,
        human_score: float,
        auto_weight: float = 0.7,
        human_weight: float = 0.3
    ) -> float:
        """Combina scores autom√°tico e humano com pesos"""
        return (auto_score * auto_weight) + (human_score * human_weight)
```

---

## 5. Exemplos Pr√°ticos Completos

### 5.1 Exemplo 1: Avalia√ß√£o de Agente RAG

```python
import asyncio
from google.adk import Agent, Runner, Session
from langfuse import Langfuse

# Configura√ß√£o
langfuse = Langfuse(
    secret_key="your-secret-key",
    public_key="your-public-key",
    host="https://cloud.langfuse.com"
)

# Judge Agent
judge_agent = Agent(
    name="rag_evaluator",
    description="Especialista em avaliar respostas de agentes RAG",
    instruction="""
    Voc√™ avalia respostas de agentes RAG considerando:
    1. Precis√£o das informa√ß√µes citadas
    2. Relev√¢ncia das fontes utilizadas
    3. Completude da resposta
    4. Clareza na apresenta√ß√£o
    
    Sempre forne√ßa JSON estruturado com scores e justificativas.
    """,
    model="gemini-2.0-flash"
)

# Criar judge
judge = LangfuseLLMJudge(
    judge_agent=judge_agent,
    runner=Runner(),
    evaluation_criteria={
        "accuracy": "As informa√ß√µes est√£o corretas?",
        "citation_quality": "As cita√ß√µes s√£o relevantes?",
        "completeness": "A resposta est√° completa?",
        "clarity": "A resposta √© clara?"
    },
    langfuse_client=langfuse
)

# Casos de teste
test_cases = [
    {
        "user_query": "Quais s√£o os principais segmentos de neg√≥cio da Alphabet?",
        "expected_response": "Google Services, Google Cloud, Other Bets",
        "expected_trajectory": ["search_documents", "retrieve_context", "generate_response"],
        "context": {"domain": "finance", "document_type": "10-K"}
    }
]

# Executar avalia√ß√£o
async def evaluate_rag_agent():
    results = []
    
    for test_case in test_cases:
        # Simula execu√ß√£o do agente RAG (substitua pelo seu agente real)
        agent_response = "A Alphabet possui tr√™s segmentos principais..."
        agent_trajectory = ["search_documents", "retrieve_context", "generate_response"]
        
        # Avalia trajet√≥ria
        traj_eval = await judge.evaluate_trajectory(
            expected_trajectory=test_case["expected_trajectory"],
            actual_trajectory=agent_trajectory,
            context=test_case.get("context")
        )
        
        # Avalia resposta
        resp_eval = await judge.evaluate_response(
            user_query=test_case["user_query"],
            agent_response=agent_response,
            expected_response=test_case.get("expected_response"),
            context=test_case.get("context")
        )
        
        results.append({
            "test_case": test_case,
            "trajectory_evaluation": traj_eval,
            "response_evaluation": resp_eval
        })
    
    return results

# Executar
# results = asyncio.run(evaluate_rag_agent())
```

### 5.2 Exemplo 2: Avalia√ß√£o Comparativa de Modelos

```python
async def compare_models(
    user_query: str,
    responses: List[Dict[str, str]],
    judge: LLMJudge
) -> Dict[str, Any]:
    """Compara respostas de diferentes modelos"""
    
    comparison = await judge.compare_responses(
        user_query=user_query,
        responses=responses
    )
    
    print(f"Melhor resposta: {comparison['winner']}")
    print(f"Scores: {comparison['scores']}")
    print(f"Racioc√≠nio: {comparison['reasoning']}")
    
    return comparison

# Uso
responses = [
    {"model": "gpt-4", "response": "Resposta do GPT-4..."},
    {"model": "gemini-2.0-flash", "response": "Resposta do Gemini..."},
    {"model": "claude-3.5-sonnet", "response": "Resposta do Claude..."}
]

comparison = asyncio.run(compare_models(
    user_query="Explique o conceito de LLMs as a Judge",
    responses=responses,
    judge=judge
))
```

### 5.3 Exemplo 3: Pipeline de Avalia√ß√£o Cont√≠nua

```python
class ContinuousEvaluationPipeline:
    """Pipeline de avalia√ß√£o cont√≠nua com LLM Judge"""
    
    def __init__(
        self,
        agent_to_evaluate: Agent,
        judge: LangfuseLLMJudge,
        test_suite: List[Dict[str, Any]]
    ):
        self.agent = agent_to_evaluate
        self.judge = judge
        self.test_suite = test_suite
        self.runner = Runner()
    
    async def run_evaluation_cycle(self) -> Dict[str, Any]:
        """Executa um ciclo completo de avalia√ß√£o"""
        
        results = []
        
        for test_case in self.test_suite:
            # Executa agente
            session = Session()
            agent_result = await self.runner.run(
                agent=self.agent,
                session=session,
                user_content=test_case["user_query"]
            )
            
            # Extrai trajet√≥ria (se dispon√≠vel)
            trajectory = self._extract_trajectory(agent_result)
            
            # Avalia com judge
            trace_id = self.judge.langfuse.get_current_trace_id()
            
            traj_eval = None
            if test_case.get("expected_trajectory"):
                traj_eval = await self.judge.evaluate_trajectory(
                    expected_trajectory=test_case["expected_trajectory"],
                    actual_trajectory=trajectory,
                    trace_id=trace_id
                )
            
            resp_eval = await self.judge.evaluate_response(
                user_query=test_case["user_query"],
                agent_response=agent_result.content,
                expected_response=test_case.get("expected_response"),
                trace_id=trace_id
            )
            
            results.append({
                "test_case_id": test_case.get("id"),
                "trajectory_evaluation": traj_eval,
                "response_evaluation": resp_eval,
                "trace_id": trace_id
            })
        
        return {
            "results": results,
            "summary": self._generate_summary(results)
        }
    
    def _extract_trajectory(self, agent_result) -> List[str]:
        """Extrai trajet√≥ria do resultado do agente"""
        # Implementar extra√ß√£o baseada na estrutura do ADK
        trajectory = []
        # Exemplo: percorrer eventos do agente
        return trajectory
    
    def _generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Gera resumo das avalia√ß√µes"""
        scores = [
            r["response_evaluation"].get("score", 0)
            for r in results
            if r.get("response_evaluation")
        ]
        
        return {
            "total_tests": len(results),
            "average_score": sum(scores) / len(scores) if scores else 0,
            "pass_rate": sum(1 for s in scores if s >= 0.7) / len(scores) if scores else 0,
            "failing_tests": [
                r["test_case_id"]
                for r in results
                if r.get("response_evaluation", {}).get("score", 0) < 0.7
            ]
        }
```

---

## 6. M√©tricas e Benchmarks

### 6.1 M√©tricas de Qualidade do Judge

#### 6.1.1 Consist√™ncia (Consistency)

Mede qu√£o consistente o judge √© em avalia√ß√µes similares:

```python
def measure_judge_consistency(
    judge: LLMJudge,
    test_cases: List[Dict[str, Any]],
    num_runs: int = 3
) -> Dict[str, float]:
    """Mede consist√™ncia do judge executando m√∫ltiplas vezes"""
    
    all_scores = []
    
    for test_case in test_cases:
        scores = []
        for _ in range(num_runs):
            eval_result = await judge.evaluate_response(
                user_query=test_case["user_query"],
                agent_response=test_case["agent_response"]
            )
            scores.append(eval_result.get("score", 0))
        
        all_scores.append(scores)
    
    # Calcula vari√¢ncia m√©dia
    variances = [np.var(scores) for scores in all_scores]
    avg_variance = np.mean(variances)
    
    # Calcula coeficiente de varia√ß√£o
    cv_scores = [
        np.std(scores) / np.mean(scores) if np.mean(scores) > 0 else 0
        for scores in all_scores
    ]
    avg_cv = np.mean(cv_scores)
    
    return {
        "average_variance": avg_variance,
        "average_coefficient_of_variation": avg_cv,
        "consistency_score": 1 - min(avg_cv, 1.0)  # 0-1, maior √© melhor
    }
```

#### 6.1.2 Correla√ß√£o com Avalia√ß√£o Humana

```python
def measure_human_correlation(
    judge: LLMJudge,
    human_evaluations: List[Dict[str, Any]]
) -> Dict[str, float]:
    """Mede correla√ß√£o entre avalia√ß√£o do judge e avalia√ß√£o humana"""
    
    from scipy.stats import pearsonr, spearmanr
    
    judge_scores = []
    human_scores = []
    
    for eval_data in human_evaluations:
        judge_eval = await judge.evaluate_response(
            user_query=eval_data["user_query"],
            agent_response=eval_data["agent_response"]
        )
        
        judge_scores.append(judge_eval.get("score", 0))
        human_scores.append(eval_data["human_score"])
    
    # Correla√ß√£o de Pearson (linear)
    pearson_corr, pearson_p = pearsonr(judge_scores, human_scores)
    
    # Correla√ß√£o de Spearman (rank)
    spearman_corr, spearman_p = spearmanr(judge_scores, human_scores)
    
    return {
        "pearson_correlation": pearson_corr,
        "pearson_p_value": pearson_p,
        "spearman_correlation": spearman_corr,
        "spearman_p_value": spearman_p,
        "agreement_rate": sum(
            1 for j, h in zip(judge_scores, human_scores)
            if abs(j - h) < 0.2
        ) / len(judge_scores)
    }
```

#### 6.1.3 Vi√©s e Justi√ßa (Bias and Fairness)

```python
def measure_judge_bias(
    judge: LLMJudge,
    test_suite: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """Mede vi√©s do judge em diferentes categorias"""
    
    categories = {}
    
    for test_case in test_suite:
        category = test_case.get("category", "unknown")
        
        if category not in categories:
            categories[category] = []
        
        eval_result = await judge.evaluate_response(
            user_query=test_case["user_query"],
            agent_response=test_case["agent_response"]
        )
        
        categories[category].append(eval_result.get("score", 0))
    
    # Calcula m√©dias por categoria
    category_means = {
        cat: np.mean(scores)
        for cat, scores in categories.items()
    }
    
    # Identifica disparidades
    overall_mean = np.mean(list(category_means.values()))
    disparities = {
        cat: mean - overall_mean
        for cat, mean in category_means.items()
    }
    
    return {
        "category_means": category_means,
        "overall_mean": overall_mean,
        "disparities": disparities,
        "max_disparity": max(abs(d) for d in disparities.values())
    }
```

### 6.2 Benchmarks Recomendados

#### 6.2.1 MT-Bench (Multi-Turn Benchmark)

Adaptado para avalia√ß√£o de agentes:

```python
MT_BENCH_CASES = [
    {
        "category": "writing",
        "user_query": "Escreva um poema sobre intelig√™ncia artificial",
        "criteria": ["creativity", "coherence", "literary_quality"]
    },
    {
        "category": "reasoning",
        "user_query": "Resolva este problema de l√≥gica: ...",
        "criteria": ["correctness", "reasoning_quality", "explanation"]
    },
    # ... mais casos
]
```

#### 6.2.2 HELM (Holistic Evaluation of Language Models)

Adaptado para agentes:

```python
HELM_SCENARIOS = [
    "question_answering",
    "summarization",
    "code_generation",
    "reasoning",
    "safety"
]
```

### 6.3 M√©tricas de Custo-Efici√™ncia

```python
def calculate_cost_efficiency(
    judge: LLMJudge,
    evaluations: List[Dict[str, Any]],
    cost_per_token: float
) -> Dict[str, float]:
    """Calcula custo-efici√™ncia do judge"""
    
    total_tokens = 0
    total_evaluations = len(evaluations)
    
    for eval_data in evaluations:
        # Estima tokens (simplificado)
        prompt_tokens = estimate_tokens(eval_data["prompt"])
        response_tokens = estimate_tokens(eval_data["response"])
        total_tokens += prompt_tokens + response_tokens
    
    total_cost = total_tokens * cost_per_token
    cost_per_evaluation = total_cost / total_evaluations
    
    return {
        "total_evaluations": total_evaluations,
        "total_tokens": total_tokens,
        "total_cost": total_cost,
        "cost_per_evaluation": cost_per_evaluation,
        "tokens_per_evaluation": total_tokens / total_evaluations
    }
```

---

## 7. Melhores Pr√°ticas

### 7.1 Design de Prompts para Judge

#### Princ√≠pios:

1. **Seja Espec√≠fico**: Defina claramente os crit√©rios de avalia√ß√£o
2. **Use Exemplos**: Inclua exemplos de boas e m√°s respostas
3. **Estruture o Output**: Solicite JSON estruturado
4. **Defina Escalas**: Especifique claramente a escala de pontua√ß√£o
5. **Pe√ßa Justificativas**: Solicite explica√ß√µes para transpar√™ncia

#### Template de Prompt Recomendado:

```python
JUDGE_PROMPT_TEMPLATE = """
Voc√™ √© um juiz especializado em avaliar {domain}.

TAREFA:
Avalie a seguinte resposta de um agente de IA.

PERGUNTA DO USU√ÅRIO:
{user_query}

RESPOSTA DO AGENTE:
{agent_response}

{optional_sections}

CRIT√âRIOS DE AVALIA√á√ÉO:
{criteria}

ESCALA DE PONTUA√á√ÉO:
- 0.0-0.3: Insatisfat√≥rio
- 0.4-0.6: Aceit√°vel
- 0.7-0.8: Bom
- 0.9-1.0: Excelente

FORMATO DE RESPOSTA:
Forne√ßa sua avalia√ß√£o em JSON com a seguinte estrutura:
{{
    "score": <float 0-1>,
    "scores_by_criterion": {{
        "criterion1": <float 0-1>,
        "criterion2": <float 0-1>
    }},
    "justification": "<explica√ß√£o detalhada>",
    "strengths": ["<ponto forte 1>", "<ponto forte 2>"],
    "weaknesses": ["<ponto fraco 1>", "<ponto fraco 2>"],
    "recommendations": ["<recomenda√ß√£o 1>", "<recomenda√ß√£o 2>"]
}}

IMPORTANTE:
- Seja objetivo e justo
- Considere o contexto fornecido
- Forne√ßa feedback construtivo
- Justifique suas pontua√ß√µes
"""
```

### 7.2 Estrat√©gias de Redu√ß√£o de Custo

1. **Caching**: Cache avalia√ß√µes de respostas id√™nticas
2. **Batching**: Agrupe m√∫ltiplas avalia√ß√µes em uma √∫nica chamada
3. **Modelos Menores**: Use modelos menores para avalia√ß√µes simples
4. **Amostragem**: Avalie apenas uma amostra representativa
5. **Avalia√ß√£o Hier√°rquica**: Use judge menor primeiro, judge maior apenas se necess√°rio

```python
class CostOptimizedJudge:
    """Judge otimizado para custo"""
    
    def __init__(
        self,
        fast_judge: LLMJudge,  # Modelo r√°pido/barato
        accurate_judge: LLMJudge,  # Modelo preciso/caro
        threshold: float = 0.7
    ):
        self.fast_judge = fast_judge
        self.accurate_judge = accurate_judge
        self.threshold = threshold
        self.cache = {}
    
    async def evaluate_with_fallback(
        self,
        user_query: str,
        agent_response: str,
        **kwargs
    ) -> Dict[str, Any]:
        """Avalia com fallback para judge mais preciso se necess√°rio"""
        
        # Verifica cache
        cache_key = f"{user_query}:{agent_response}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # Avalia com judge r√°pido primeiro
        fast_eval = await self.fast_judge.evaluate_response(
            user_query=user_query,
            agent_response=agent_response,
            **kwargs
        )
        
        fast_score = fast_eval.get("score", 0)
        
        # Se score est√° pr√≥ximo do threshold, usa judge preciso
        if abs(fast_score - self.threshold) < 0.1:
            accurate_eval = await self.accurate_judge.evaluate_response(
                user_query=user_query,
                agent_response=agent_response,
                **kwargs
            )
            result = accurate_eval
        else:
            result = fast_eval
        
        # Cache resultado
        self.cache[cache_key] = result
        
        return result
```

### 7.3 Tratamento de Erros e Edge Cases

```python
class RobustJudge(LLMJudge):
    """Judge com tratamento robusto de erros"""
    
    async def evaluate_response(
        self,
        user_query: str,
        agent_response: str,
        max_retries: int = 3,
        **kwargs
    ) -> Dict[str, Any]:
        """Avalia com retry e fallback"""
        
        for attempt in range(max_retries):
            try:
                result = await super().evaluate_response(
                    user_query=user_query,
                    agent_response=agent_response,
                    **kwargs
                )
                
                # Valida resultado
                if self._validate_result(result):
                    return result
                else:
                    raise ValueError("Resultado inv√°lido")
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    # Fallback: retorna avalia√ß√£o b√°sica
                    return self._fallback_evaluation(
                        user_query=user_query,
                        agent_response=agent_response,
                        error=str(e)
                    )
                
                # Aguarda antes de retry
                await asyncio.sleep(2 ** attempt)
        
        raise RuntimeError("Falha ao avaliar ap√≥s m√∫ltiplas tentativas")
    
    def _validate_result(self, result: Dict[str, Any]) -> bool:
        """Valida estrutura do resultado"""
        required_fields = ["score", "justification"]
        return all(field in result for field in required_fields) and \
               0 <= result.get("score", -1) <= 1
    
    def _fallback_evaluation(
        self,
        user_query: str,
        agent_response: str,
        error: str
    ) -> Dict[str, Any]:
        """Avalia√ß√£o b√°sica de fallback"""
        return {
            "score": 0.5,  # Score neutro
            "justification": f"Avalia√ß√£o autom√°tica falhou: {error}",
            "error": True,
            "fallback": True
        }
```

### 7.4 Calibra√ß√£o do Judge

```python
class CalibratedJudge(LLMJudge):
    """Judge calibrado com dados de refer√™ncia"""
    
    def __init__(self, *args, calibration_data: List[Dict[str, Any]] = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.calibration_data = calibration_data or []
        self.calibration_factor = self._calculate_calibration_factor()
    
    def _calculate_calibration_factor(self) -> float:
        """Calcula fator de calibra√ß√£o baseado em dados de refer√™ncia"""
        if not self.calibration_data:
            return 1.0
        
        # Executa avalia√ß√µes e compara com refer√™ncia
        # Retorna fator de ajuste
        return 1.0  # Simplificado
    
    async def evaluate_response(self, *args, **kwargs) -> Dict[str, Any]:
        """Avalia com calibra√ß√£o aplicada"""
        result = await super().evaluate_response(*args, **kwargs)
        
        # Aplica calibra√ß√£o
        if "score" in result:
            result["score"] = min(1.0, result["score"] * self.calibration_factor)
            result["original_score"] = result["score"] / self.calibration_factor
        
        return result
```

---

## 8. Casos de Uso Espec√≠ficos

### 8.1 Avalia√ß√£o de Agentes Conversacionais

```python
CONVERSATIONAL_JUDGE_CRITERIA = {
    "coherence": "A resposta √© coerente com o contexto da conversa?",
    "relevance": "A resposta √© relevante para a pergunta do usu√°rio?",
    "helpfulness": "A resposta √© √∫til para o usu√°rio?",
    "naturalness": "A resposta soa natural e conversacional?",
    "completeness": "A resposta est√° completa ou precisa de follow-up?"
}
```

### 8.2 Avalia√ß√£o de Agentes de C√≥digo

```python
CODE_JUDGE_CRITERIA = {
    "correctness": "O c√≥digo est√° correto e funciona?",
    "efficiency": "O c√≥digo √© eficiente?",
    "readability": "O c√≥digo √© leg√≠vel e bem estruturado?",
    "best_practices": "O c√≥digo segue melhores pr√°ticas?",
    "documentation": "O c√≥digo est√° bem documentado?"
}
```

### 8.3 Avalia√ß√£o de Agentes RAG

```python
RAG_JUDGE_CRITERIA = {
    "answer_quality": "A resposta est√° correta e completa?",
    "source_relevance": "As fontes s√£o relevantes?",
    "citation_accuracy": "As cita√ß√µes est√£o corretas?",
    "groundedness": "A resposta est√° fundamentada nas fontes?",
    "attribution": "A atribui√ß√£o √†s fontes est√° clara?"
}
```

---

## 9. Conclus√µes e Recomenda√ß√µes

### 9.1 Resumo das Recomenda√ß√µes

#### Para Projetos com Google ADK:

1. **Judge Principal**: Gemini 2.0 Flash
   - Excelente integra√ß√£o com ADK
   - Custo-benef√≠cio ideal
   - Lat√™ncia baixa

2. **Judge para Casos Cr√≠ticos**: Gemini 2.0 Pro ou GPT-4o
   - Maior capacidade de racioc√≠nio
   - Maior consist√™ncia

3. **Integra√ß√£o**: Use Langfuse para tracing e analytics
   - Rastreamento completo
   - Feedback loops
   - An√°lise de performance

#### Estrat√©gia de Implementa√ß√£o:

1. **Fase 1**: Implementar judge b√°sico com Gemini 2.0 Flash
2. **Fase 2**: Integrar com Langfuse para observabilidade
3. **Fase 3**: Adicionar avalia√ß√£o comparativa e benchmarks
4. **Fase 4**: Otimizar custos e performance
5. **Fase 5**: Calibrar com dados de produ√ß√£o

### 9.2 Pr√≥ximos Passos

1. **Implementar POC**: Criar prova de conceito com judge b√°sico
2. **Coletar Dados**: Executar avalia√ß√µes em casos reais
3. **Calibrar**: Ajustar judge com feedback humano
4. **Escalar**: Expandir para mais casos de uso
5. **Monitorar**: Acompanhar performance e custos

### 9.3 Recursos Adicionais

- [Google ADK Documentation](https://github.com/google/labs-adk)
- [Langfuse Documentation](https://langfuse.com/docs)
- [MT-Bench Paper](https://arxiv.org/abs/2306.05685)
- [HELM Benchmark](https://crfm.stanford.edu/helm/)

---

## 10. Ap√™ndices

### 10.1 Exemplo Completo de Implementa√ß√£o

Ver arquivo `examples/complete_judge_implementation.py` para exemplo completo.

### 10.2 Templates de Prompts

Ver arquivo `templates/judge_prompts.py` para templates reutiliz√°veis.

### 10.3 Configura√ß√µes Recomendadas

Ver arquivo `configs/judge_configs.yaml` para configura√ß√µes recomendadas.

---

**Documento criado em:** 2024  
**√öltima atualiza√ß√£o:** 2024  
**Autor:** Equipe de IA  
**Vers√£o:** 1.0

